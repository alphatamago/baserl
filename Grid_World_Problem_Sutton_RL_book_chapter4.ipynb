{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "This notebook is about Grid World Problem, as described in Sutton's Reinforcement Learning book, 2nd edition, page 61.\n",
    "\n",
    "Here is a summary of the problem:\n",
    "\n",
    "4x4 grid world. Locations (0,0) and (3,3) are terminal, all others non-terminal.\n",
    "Four actions possible from each state: U(p), D(own), L(eft), R(ight).\n",
    "If an action would lead out of grid, the current state does not change.\n",
    "This is an undiscounted, episodic task. The reward is -1 on all transitions until the terminal state is reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from baserl.common import *\n",
    "from baserl.grid_world import GridWorld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial value function:\n",
      "0.00 0.00 0.00 0.00 \n",
      "0.00 0.00 0.00 0.00 \n",
      "0.00 0.00 0.00 0.00 \n",
      "0.00 0.00 0.00 0.00 \n",
      "\n",
      "Initial greedy policy:\n",
      "UDRL UDRL UDRL UDRL \n",
      "UDRL UDRL UDRL UDRL \n",
      "UDRL UDRL UDRL UDRL \n",
      "UDRL UDRL UDRL UDRL \n",
      "\n",
      "value function at iteration 5\n",
      "0.00 -4.49 -6.30 -6.91 \n",
      "-4.49 -6.26 -7.22 -7.37 \n",
      "-6.30 -7.22 -7.19 -5.93 \n",
      "-6.91 -7.37 -5.93 0.00 \n",
      "\n",
      "greedy policy at iteration 5\n",
      "  UL    L    L    L \n",
      "   U   UL    L    D \n",
      "   U    U   DR    D \n",
      "   U    R    R   DR \n",
      "\n",
      "iterative_policy_evaluation: num iter= 5 delta= 1.3317079544067383\n",
      "value function at iteration 10\n",
      "0.00 -7.83 -11.12 -12.23 \n",
      "-7.83 -10.42 -11.77 -11.86 \n",
      "-11.12 -11.77 -11.05 -8.81 \n",
      "-12.23 -11.86 -8.81 0.00 \n",
      "\n",
      "greedy policy at iteration 10\n",
      "  UL    L    L    L \n",
      "   U   UL    L    D \n",
      "   U    U   DR    D \n",
      "   U    R    R   DR \n",
      "\n",
      "iterative_policy_evaluation: num iter= 10 delta= 0.8924059502996897\n",
      "value function at iteration 15\n",
      "0.00 -10.01 -14.27 -15.69 \n",
      "-10.01 -13.11 -14.69 -14.75 \n",
      "-14.27 -14.69 -13.52 -10.65 \n",
      "-15.69 -14.75 -10.65 0.00 \n",
      "\n",
      "greedy policy at iteration 15\n",
      "  UL    L    L    L \n",
      "   U   UL    L    D \n",
      "   U    U   DR    D \n",
      "   U    R    R   DR \n",
      "\n",
      "iterative_policy_evaluation: num iter= 15 delta= 0.5771221272796652\n",
      "value function at iteration 20\n",
      "0.00 -11.43 -16.30 -17.93 \n",
      "-11.43 -14.84 -16.57 -16.61 \n",
      "-16.30 -16.57 -15.11 -11.84 \n",
      "-17.93 -16.61 -11.84 0.00 \n",
      "\n",
      "greedy policy at iteration 20\n",
      "  UL    L    L    L \n",
      "   U   UL    L    D \n",
      "   U    U   DR    D \n",
      "   U    R    R   DR \n",
      "\n",
      "iterative_policy_evaluation: num iter= 20 delta= 0.372594031842965\n",
      "value function at iteration 25\n",
      "0.00 -12.34 -17.61 -19.37 \n",
      "-12.34 -15.96 -17.79 -17.81 \n",
      "-17.61 -17.79 -16.13 -12.61 \n",
      "-19.37 -17.81 -12.61 0.00 \n",
      "\n",
      "greedy policy at iteration 25\n",
      "  UL    L    L    L \n",
      "   U   UL    L    D \n",
      "   U    U   DR    D \n",
      "   U    R    R   DR \n",
      "\n",
      "iterative_policy_evaluation: num iter= 25 delta= 0.24052953664261167\n",
      "value function at iteration 30\n",
      "0.00 -12.93 -18.46 -20.30 \n",
      "-12.93 -16.68 -18.57 -18.59 \n",
      "-18.46 -18.57 -16.79 -13.10 \n",
      "-20.30 -18.59 -13.10 0.00 \n",
      "\n",
      "greedy policy at iteration 30\n",
      "  UL    L    L    L \n",
      "   U   UL    L    D \n",
      "   U    U   DR    D \n",
      "   U    R    R   DR \n",
      "\n",
      "iterative_policy_evaluation: num iter= 30 delta= 0.1552741675450946\n",
      "value function at iteration 35\n",
      "0.00 -13.31 -19.00 -20.90 \n",
      "-13.31 -17.15 -19.08 -19.09 \n",
      "-19.00 -19.08 -17.22 -13.42 \n",
      "-20.90 -19.09 -13.42 0.00 \n",
      "\n",
      "greedy policy at iteration 35\n",
      "  UL    L    L    L \n",
      "   U   UL    L    D \n",
      "   U    U   DR    D \n",
      "   U    R    R   DR \n",
      "\n",
      "iterative_policy_evaluation: num iter= 35 delta= 0.10023742949022107\n",
      "value function at iteration 40\n",
      "0.00 -13.55 -19.36 -21.29 \n",
      "-13.55 -17.45 -19.40 -19.41 \n",
      "-19.36 -19.40 -17.50 -13.62 \n",
      "-21.29 -19.41 -13.62 0.00 \n",
      "\n",
      "greedy policy at iteration 40\n",
      "  UL    L    L    L \n",
      "   U   UL    L    D \n",
      "   U    U   DR    D \n",
      "   U    R    R   DR \n",
      "\n",
      "iterative_policy_evaluation: num iter= 40 delta= 0.06470839506731707\n",
      "value function at iteration 45\n",
      "0.00 -13.71 -19.59 -21.54 \n",
      "-13.71 -17.65 -19.62 -19.62 \n",
      "-19.59 -19.62 -17.68 -13.76 \n",
      "-21.54 -19.62 -13.76 0.00 \n",
      "\n",
      "greedy policy at iteration 45\n",
      "  UL    L    L    L \n",
      "   U   UL    L    D \n",
      "   U    U   DR    D \n",
      "   U    R    R   DR \n",
      "\n",
      "iterative_policy_evaluation: num iter= 45 delta= 0.041772583471018976\n",
      "value function at iteration 50\n",
      "0.00 -13.81 -19.73 -21.71 \n",
      "-13.81 -17.77 -19.75 -19.75 \n",
      "-19.73 -19.75 -17.79 -13.84 \n",
      "-21.71 -19.75 -13.84 0.00 \n",
      "\n",
      "greedy policy at iteration 50\n",
      "  UL    L    L    L \n",
      "   U   UL    L    D \n",
      "   U    U   DR    D \n",
      "   U    R    R   DR \n",
      "\n",
      "iterative_policy_evaluation: num iter= 50 delta= 0.026966342280470457\n",
      "value function at iteration 55\n",
      "0.00 -13.88 -19.83 -21.81 \n",
      "-13.88 -17.85 -19.84 -19.84 \n",
      "-19.83 -19.84 -17.86 -13.90 \n",
      "-21.81 -19.84 -13.90 0.00 \n",
      "\n",
      "greedy policy at iteration 55\n",
      "  UL    L    L    L \n",
      "   U   UL    L    D \n",
      "   U    U   DR    D \n",
      "   U    R    R   DR \n",
      "\n",
      "iterative_policy_evaluation: num iter= 55 delta= 0.017408155195646202\n",
      "value function at iteration 60\n",
      "0.00 -13.92 -19.89 -21.88 \n",
      "-13.92 -17.90 -19.90 -19.90 \n",
      "-19.89 -19.90 -17.91 -13.93 \n",
      "-21.88 -19.90 -13.93 0.00 \n",
      "\n",
      "greedy policy at iteration 60\n",
      "  UL    L    L    L \n",
      "   U   UL    L    D \n",
      "   U    U   DR    D \n",
      "   U    R    R   DR \n",
      "\n",
      "iterative_policy_evaluation: num iter= 60 delta= 0.011237855848740708\n",
      "value function at iteration 65\n",
      "0.00 -13.95 -19.93 -21.92 \n",
      "-13.95 -17.94 -19.93 -19.93 \n",
      "-19.93 -19.93 -17.94 -13.96 \n",
      "-21.92 -19.93 -13.96 0.00 \n",
      "\n",
      "greedy policy at iteration 65\n",
      "  UL    L    L    L \n",
      "   U   UL    L    D \n",
      "   U    U   DR    D \n",
      "   U    R    R   DR \n",
      "\n",
      "iterative_policy_evaluation: num iter= 65 delta= 0.007254611568992431\n",
      "value function at iteration 70\n",
      "0.00 -13.97 -19.95 -21.95 \n",
      "-13.97 -17.96 -19.96 -19.96 \n",
      "-19.95 -19.96 -17.96 -13.97 \n",
      "-21.95 -19.96 -13.97 0.00 \n",
      "\n",
      "greedy policy at iteration 70\n",
      "  UL    L    L    L \n",
      "   U   UL    L    D \n",
      "   U    U   DR    D \n",
      "   U    R    R   DR \n",
      "\n",
      "iterative_policy_evaluation: num iter= 70 delta= 0.004683223359091215\n",
      "value function at iteration 75\n",
      "0.00 -13.98 -19.97 -21.97 \n",
      "-13.98 -17.97 -19.97 -19.97 \n",
      "-19.97 -19.97 -17.98 -13.98 \n",
      "-21.97 -19.97 -13.98 0.00 \n",
      "\n",
      "greedy policy at iteration 75\n",
      "  UL    L    L    L \n",
      "   U   UL    L    D \n",
      "   U    U   DR    D \n",
      "   U    R    R   DR \n",
      "\n",
      "iterative_policy_evaluation: num iter= 75 delta= 0.003023260559515961\n",
      "value function at iteration 80\n",
      "0.00 -13.99 -19.98 -21.98 \n",
      "-13.99 -17.98 -19.98 -19.98 \n",
      "-19.98 -19.98 -17.98 -13.99 \n",
      "-21.98 -19.98 -13.99 0.00 \n",
      "\n",
      "greedy policy at iteration 80\n",
      "  UL    L    L    L \n",
      "   U   UL    L    D \n",
      "   U    U   DR    D \n",
      "   U    R    R   DR \n",
      "\n",
      "iterative_policy_evaluation: num iter= 80 delta= 0.0019516695467842737\n",
      "value function at iteration 85\n",
      "0.00 -13.99 -19.99 -21.99 \n",
      "-13.99 -17.99 -19.99 -19.99 \n",
      "-19.99 -19.99 -17.99 -13.99 \n",
      "-21.99 -19.99 -13.99 0.00 \n",
      "\n",
      "greedy policy at iteration 85\n",
      "  UL    L    L    L \n",
      "   U   UL    L    D \n",
      "   U    U   DR    D \n",
      "   U    R    R   DR \n",
      "\n",
      "iterative_policy_evaluation: num iter= 85 delta= 0.0012599026596831209\n",
      "Final value function after #iters = 88\n",
      "0.00 -13.99 -19.99 -21.99 \n",
      "-13.99 -17.99 -19.99 -19.99 \n",
      "-19.99 -19.99 -17.99 -13.99 \n",
      "-21.99 -19.99 -13.99 0.00 \n",
      "Final greedy policy after #iters = 88\n",
      "  UL    L    L    L \n",
      "   U   UL    L    D \n",
      "   U    U   DR    D \n",
      "   U    R    R   DR \n",
      "iterative_policy_evaluation num_iter: 88\n"
     ]
    }
   ],
   "source": [
    "# Applying Iterative Policy Evaluation starting from the random policy, to reproduce the results from the RL book.\n",
    "# The optimal policy (as determined by the greedy policy built from the current value function) is reached very fast, in just\n",
    "# a couple of iterations, while it takes 88 or so iterations to compute the accurate value function for this optimal policy.\n",
    "# As noted in the book, an optimal value function is not needed to reach a good/optimal policy.\n",
    "mdp = GridWorld()\n",
    "ranom_policy = make_random_policy(mdp.states(), mdp.actions)\n",
    "v = iterative_policy_evaluation(ranom_policy,\n",
    "                            theta=0.001,\n",
    "                            states=mdp.states(),\n",
    "                            is_terminal=mdp.is_terminal,\n",
    "                            actions=mdp.actions,\n",
    "                            transitions=mdp.transitions,\n",
    "                            gamma=1.0,\n",
    "                            in_place=True,\n",
    "                            max_iter=100,\n",
    "                            print_value=mdp.print_value,\n",
    "                            print_policy=mdp.print_policy,\n",
    "                            print_every_n=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delta at iteration: 1 1.0\n",
      "delta at iteration: 2 1.0\n",
      "delta at iteration: 3 1.0\n",
      "delta at iteration: 4 0\n",
      "value function at iteration 4\n",
      "0.00 -1.00 -2.00 -3.00 \n",
      "-1.00 -2.00 -3.00 -2.00 \n",
      "-2.00 -3.00 -2.00 -1.00 \n",
      "-3.00 -2.00 -1.00 0.00 \n",
      "\n",
      "policy:\n",
      "  UL    L    L   DL \n",
      "   U   UL UDRL    D \n",
      "   U UDRL   DR    D \n",
      "  UR    R    R   DR \n",
      "\n",
      "Done in time: 0.0015058517456054688\n"
     ]
    }
   ],
   "source": [
    "# Let's also run Value Iteration - this is not required in the book.\n",
    "mdp = GridWorld()\n",
    "start_time = time.time()\n",
    "mdp_policy, mdp_v = value_iteration(\n",
    "        states=mdp.states(), \n",
    "        is_terminal=mdp.is_terminal, \n",
    "        actions=mdp.actions,\n",
    "        transitions=mdp.transitions,\n",
    "        gamma=mdp.gamma(),\n",
    "        delta_threshold=0.001,\n",
    "        max_iter=100,\n",
    "        print_value=mdp.print_value,\n",
    "        print_policy=mdp.print_policy)\n",
    "print(\"Done in time:\", time.time()-start_time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
